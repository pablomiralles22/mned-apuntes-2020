\section{Preliminares}

\subsection{Problemas de valor inicial}

%TODO cambiar esta label y actualizar referencias
\begin{definition}\label{eqn:pvi}
    Un \emph{problema de valor inicial} (en adelante, PVI)
    es una ecuación diferencial ordinaria dada por
    $\vf : \Omega \subset \R \times \R^n \to \R^n$
    acompañada de una condición inicial $(t_0, \vy_0)$ del dominio de $\vf$
    del que se busca una solución $\vy$ que cumpla
    \begin{equation*}
        \left\{
        \begin{aligned}
            \vy'(t) &= \vf(t, \vy(t)) \\
            \vy(t_0) &= \vy_0 \\
        \end{aligned}.
        \right.
    \end{equation*}
\end{definition}

%TODO la definición no deja cuestionarse el orden, no?
% \begin{remark}
%     Los problemas que trataremos serán de orden $1$ siempre,
%     en otro caso los transformaremos en un problema de orden $1$.
% \end{remark}

A lo largo del texto hablaremos de la solución $\vy$ de un PVI,
por lo que estaremos asumiendo que existe una solución y que es única.
Esto será así en la mayoría de problemas que queramos resolver,
a poco que la función $\vf$ del PVI cumpla unos requisitos mínimos.

\begin{theorem}[Picard-Lindelöf]
    Sea $\vf(t, \vy) : \Omega \subset \R \times \R^n \to \R^n$,
    donde $\Omega$ es abierto,
    una función continua y
    localmente Lipschitziana respecto de la segunda variable.
    Entonces, dado $(t_0, \vy_0) \in \Omega$,
    existe un intervalo cerrado $[t_0 - h, t_0 + h]$
    donde existe una única solución del PVI
    \begin{equation*}
        \left\{
        \begin{aligned}
            \vy'(t) &= \vf(t, \vy(t)) \\
            \vy(t_0) &= \vy_0 \\
        \end{aligned}.
        \right.
    \end{equation*}
    que cumple que $(t, \vy(t)) \in \Omega$ para todo $t \in [t_0 - h, t_0 + h]$.
\end{theorem}

\begin{proof}
    La prueba de este teorema corresponde a
    un curso previo de ecuaciones diferenciales.
\end{proof}

\begin{remark}
    La condición de que $\vf$ sea Lipschitziana no es necesariamente
    muy restrictiva.
    Pensemos que es suficiente con que $\vf$ está definida sobre un convexo y
    que tenga derivada con respecto a la segunda variable continua y acotada.
    Lo que también es cierto si ese convexo es compacto.
\end{remark}

\subsection{Notación}

%TODO habría que considerar que t_i también se aproxima por tilde{t}_i, no?
Los métodos de integración que veremos calcularán soluciones de un PVI
como una lista de pares $\{(t_0, \vw_0), \ldots, (t_n, \vw_n)\}$,
donde los $t_i$ son reales de forma que $t_0 < t_1 < \cdots < t_n$
y cada punto $\vw_i$ es una aproximación de $\vy(t_i)$,
$\vw_i \approx \vy(t_i)$.
No obstante, una implementación utilizaría números de precisión finita,
produciendo una lista $\{t_0, \tw_0), \ldots, (t_n, \tw_n)\}$,
de forma que
\begin{equation*}
    \tw_i \approx \vw_i \approx \vy(t_i).
\end{equation*}

Para ver de un vistazo la notación utilizada durante todo el texto,
véase el \cref{sec:notation}.

\section{Métodos de paso fijo}

\begin{definition}
    Un método de integración es \emph{de paso fijo $h$} si la solución
    $\{(t_i, \vw_i)\}_{i = 0,\ldots,n}$ que genera cumple
    \begin{equation*}
        t_i = t_{i-1} + h \qq{para todo $i = 1,\ldots,n$.}
    \end{equation*}
\end{definition}

Cuando resolvamos un PVI, generaremos una cantidad finita de puntos.
A menudo, nos interesará conocer la solución en un intervalo $[a, b]$,
$t_0 = a$,
para lo cual lo normal es generar puntos hasta que $t_n > b$.
En general, asumiremos que $t_n = b$.
O mejor dicho, el lector puede entender que $a$ representa $t_0$
y $b$ es $t_n$ para la solución concreta que estemos tratando.

\subsection{El método de Euler}

\begin{method}\label{met:euler}
    El \emph{método de Euler} es un es un método de paso fijo $h$ que
    a partir de un estado $(t_i, \vw_i) \approx (t_i, \vy(t_i))$,
    aproxima $\vy(t_i + h)$ como
    \begin{equation*}
        \vw_{i+1} = \vw_i + h\cdot \vf(t_i, \vw_i).
    \end{equation*}
\end{method}

A continuación presentamos una desigualdad elemental que utilizaremos
en la estimación del error del método de Euler.

\begin{lemma}\label{lma:tangent-exponential}
    Se cumplen las siguientes desigualdades:
    \begin{enumerate}
        \item $1 + x \le e^x$ para todo $x \in \R$.
        \item $0 \le (1+x)^m \le e^{mx}$ para todo $m > 0$ y $x \ge -1$.
    \end{enumerate} 
\end{lemma}

\begin{proof}
    Es claro que la segunda desigualdad es consecuencia de la primera.
    La primera tiene una demostración geométrica sencilla,
    puesto que $1 + x$ es la línea tangente en $x = 0$ a $y = \nume^x$,
    y las tangentes se encuentran por debajo de una función convexa.
    Resultado que se puede probar usando un desarrollo de Taylor.
    \begin{equation*}
        \nume^x = 1 + x + \frac{x^2}{2}\nume^\xi \qq{con $\abs{\xi} < \abs{t}.$}
        \implies \nume^x \ge 1+ x.
    \end{equation*}
    Donde hemos usado que $\frac{x^2}{2}\nume^\xi \ge 0$.
\end{proof}

%TODO discutir sobre la convexidad en este resultado
\begin{theorem}[Convergencia del método de Euler]\label{thm:euler-convergence}
    Sean $f : D \subset \R \times \R \to \R$ con $D$ abierto convexo
    y $\{(t_i, w_i)\}_{i = 0,\ldots, n}$
    la solución calculada por el método de Euler para cierto paso fijo $h$
    para un PVI definido por $f$.
    Sea $y : [a, b] \to \R$ una solución del mismo PVI,
    con $(t, y(t)) \in D$, $\forall t \in [a, b]$,
    y $[t_0, t_n] \subset [a, b]$.
    Si se cumple que
    \begin{enumerate}[label=(\alph*)]
        \item $f$ es Lipschitziana respecto de la segunda variable en $D$,
        con constante de Lipschitz $L$.
        \item $y''$ existe en todo $(a, b)$ y
        su módulo está acotado por una constante $C \ge 0$.
        \item $(t_i, w_i) \in D$ para todo $i = 0,\dots,n$.
    \end{enumerate}
    entonces
    \begin{equation*}
        \max_{0 \le i \le n} \abs{y(t_i)-w_i} \le
        \nume^{(b-a)L}\abs{y(t_0) - w_0} + \qty[\frac{\nume^{(b-a)L}-1}{2L}]hC.
    \end{equation*}
\end{theorem}

\begin{proof}
    \newcommand{\err}{\operatorname{error}}

    Haciendo un desarrollo de orden uno de $y$ en $t_i$ se tiene
    \begin{gather*}
        y(t_{i+1}) = y(t_i) + hy'(t_i) + \frac{1}{2}h^2y''(\xi) =
            y(t_i) + hf(t_i, y(t_i)) + \frac{1}{2}h^2y''(\xi), \\
        w_{i+1} = w_i + hf(t_i, w_i).
    \end{gather*}
    Lo que implica que la diferencia
    \begin{equation*}
        y(t_{i+1}) - w_{i+1} =
        \qty\big(y(t_i) - w_i) + h\qty\big(f(t_i, y(t_i)) - f(t_i, w_i))
            + \frac{1}{2}h^2y''(\xi).
    \end{equation*}
    De esta última expresión se deduce,
    llamando $\err_i = \abs{y(t_i) - w_i}$ para $0 \le i \le n$
    y teniendo en cuenta que $\abs{\frac{1}{2}h^2y''(\xi)} \le \frac{1}{2}h^2C$,
    \begin{multline*}
        \err_{i+1} = \abs{y(t_{i+1}) - w_{i+1}} \le
        \abs{y(t_i) - w_i} + h\overset{\le L\abs{y(t_i) - w_i}}{
            \overbrace{\abs{f(t_i, y(t_i)) - f(t_i, w_i)}}
        }  + \frac{1}{2}h^2C \le \\
        (1 + hL)\abs{y(t_i) - w_i} + \frac{1}{2}h^2C =
        (1 + hL)\err_i + \frac{1}{2}h^2C.
    \end{multline*}

    Si ahora utilizamos esta desigualdad múltiples veces,
    podemos escribir el error como
    \begin{align*}
        \err_i \le {} & (1 + hL)\err_{i-1} + \frac{1}{2}h^2C \\
        \le {} & (1 + hL)\qty[(1 + hL)\err_{i-2} + \frac{1}{2}h^2C]
            + \frac{1}{2}h^2C \le \cdots \\
        \cdots \le {} & (1 + hL)^i\err_0
            + \overset{\textnormal{serie geométrica entre $0$ e $i-1$}}{
                \overbrace{\qty[
                    1 + (1 + hL) + (1 + hL)^2 + \cdots + (1 + hL)^{i-1}
                ]}
            } \frac{1}{2}h^2C \\
        = {} & (1 + hL)^i \err_0
            + \qty[\frac{(1 + hL)^i - 1}{hL}] \frac{1}{2}h^2C \\
        = {} & (1 + hL)^i \err_0
            + \qty[\frac{(1 + hL)^i - 1}{2L}] hC,
    \end{align*}
    y usando la desigualdad del \cref{lma:tangent-exponential},
    obtenemos la fórmula del enunciado
    \begin{equation*}
        \err_i \le
        \nume^{hLi}\err_0 + \qty[\frac{\nume^{hLi} - 1}{2L}] hC \le
        \nume^{(b-a)L}\abs{y(t_0) - w_0} + \qty[\frac{\nume^{(b-a)L}-1}{2L}]hC.
        \qedhere
    \end{equation*}
\end{proof}

\begin{theorem}[Convergencia del método de Euler con error de redondeo]
    \renewcommand{\tw}{\widetilde{w}}

    Sean $f : D \subset \R \times \R \to \R$ con $D$ abierto convexo
    e $y : [a, b] \to \R$ una solución del un PVI dado por $f$,
    con $(t, y(t)) \in D$, $\forall t \in [a, b]$,
    y $[t_0, t_n] \subset [a, b]$.
    Fijado $h > 0$, consideramos la solución numérica
    \begin{equation}
    \begin{cases}
        \tw_0 = y_0 + \delta_0 \\
        \tw_{i+1} = \tw_i + hf(t_i, \tw_i) + \delta_{i+1}
            & \text{para $i = 0,\ldots,n-1$.}
    \end{cases}
    \end{equation}
    con $\delta_i < \delta$ para todo $i = 0,\ldots,n$.
    Si se cumple que
    \begin{enumerate}[label=(\alph*)]
        \item $f$ es Lipschitziana respecto de la segunda variable en $D$,
        con constante de Lipschitz $L$.
        \item $y''$ existe en todo $(a, b)$ y
        su módulo está acotado por una constante $C \ge 0$.
        \item $(t_i, \tw_i) \in D$ para todo $i = 0,\dots,n$.
    \end{enumerate}
    entonces
    \begin{equation*}
        \max_{0 \le i \le n} \abs{y(t_i) - \tw_i} \le
        \nume^{(b-a)L}\abs{y(t_0)-\tw_0} + \qty[\frac{\nume^{(b-a)L}-1}{L}]\qty(
            \frac{1}{2}hC + \frac{\delta}{h}
        ).
    \end{equation*}
\end{theorem}

Una idea intuitiva es que
conforme reducimos el paso con el que resolvemos un PVI,
el error disminuye cada vez en menor medida,
de forma que podemos comparar dos soluciones obtenidas con pasos similares
para intentar estimar el error de una de ellas.
El siguiente teorema trata esta idea e incluso
nos permite obtener un método mejor a partir del método de Euler.

%TODO este teorema de dónde sale?
% Sería cierta la versión vectorial?
\begin{theorem}
    Dado un PVI definido por $f$
    y la solución $\{(t_i, w_i)\}_{i = 0,\ldots, n}$
    obtenida por el método de Euler,
    si la solución real $y : [t_0, t_n] \to \R$ es de clase $C^3$
    y $\pdv{f}{y},\pdv[2]{f}{y}$ son continuas,
    entonces
    \begin{equation}
        y(t_i) - w_i = hd(t_i) + \vo(h^2) \qq{para todo $t \in [t_0,b]$,}
    \end{equation}
    dónde $d$ es solución del PVI
    \begin{equation}
    \begin{cases}
        d'(t) \, = g(t)d(t) + \frac{1}{2}y''(t),
            & g(t) = \pdv{f(t,y)}{y}\qty(t, y(t)) \\
        d(t_0) = 0
    \end{cases}
    \end{equation}
\end{theorem}

\begin{proof}
    %TODO una buena referencia a la bibliografía o algo.
    Puede encontrarse en el libro de Atkinson, pero no entra.
\end{proof}

\begin{remark}
    Cuando $d(t)$ se puede obtener explícitamente,
    el término $hd(t_i)$ de la fórmula normalmente proporciona
    una estimación bastante buena del error real cometido, $\abs{y(t_i) - w_i}$,
    y la precisión de la estimación mejora si se disminuye el tamaño del paso.
\end{remark}

%TODO el remark parece decir que es práctico... te refieres a sencillo?
No suele ser muy práctico encontrar $d$,
pero el resultado nos ayuda a demostrar el siguiente teorema.

\begin{theorem}
    \newcommand{\hh}{\sfrac{h}{2}}

    En condiciones similares a las del teorema anterior,
    dadas $\{t_i^h, w_i^h\}_{i = 1,\ldots,n}$
    y $\{t_i^{\hh}, w_i^{\hh}\}_{i = 1,\ldots,2n}$,
    dos soluciones obtenidas con el método de Euler
    con pasos $h$ y $\hh$ respectivamente,
    se cumple que
    \begin{enumerate}[label=(\alph*)]
        \item $y(t_i^h) - w_{2i}^{\hh} = (w_{2i}^{\hh} - w_i^h) + \vo(h^2)$.
        \item $w_i := 2\cdot w_{2i}^{\hh} - w_i^h$
        es una aproximación de $y$ de orden $\vo(h^2)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \newcommand{\hh}{\sfrac{h}{2}}

    Aplicando el teorema anterior obtenemos
    \begin{align*}
        y(t_i^h) - w_i^h = hd(t_i^h) + \vo(h^2),
            & \qq{para todo $i = 1,\ldots,n$,} \\
        y(t_{2i}^{\hh}) - w_{2i}^{\hh} = \tfrac{h}{2}d(t_{2i}^{\hh})
            + \vo((\tfrac{h}{2})^2),
            & \qq{para todo $i = 1,\ldots,2n$.}
    \end{align*}
    Basta observar que $t_i^h = t_{2i}^{\hh}$
    y restar la primera ecuación a la segunda multiplicada por $2$.
    Despejando se obtiene tanto (a) como (b).
\end{proof}

\subsection{Estimación general del error}

Como hemos visto, a partir de conocer el error cometido en cada paso
hemos logrado estimar globalmente
el error de la solución generada por el método de Euler.
Antes de continuar nuestro estudio de más métodos de paso fijo,
vamos a definir el concepto de error local de truncamiento,
que iremos calculando para cada método.
A continuación incluímos la definición para un método general.

% \begin{definition}
%     Sean $f : \Omega \subset \R \times \R^n$ la función de un PVI
%     $p_i = (t_i, \vw_i)$ un estado del dominio de $f$,
%     $p_{i+1} = (t_{i+1}, \vw_{i+1})$
%     el siguiente estado generado por un método numérico e
%     $\vy_i$ una solución del PVI dado por $f$ y $p_i$.

%     Se define \emph{el error local de truncamiento de la estimación} como
%     \begin{equation*}
%         Z_{i+1} = \frac{\vy_i(t_{i+1}) - \vw_{i+1}}{t_{i+1} - t_i}.
%     \end{equation*}
% \end{definition}

La mayoría de métodos numéricos permiten obtener aproximaciones más precisas
a costa de realizar más cálculos.
Por ejemplo, el método de Euler nos permite
obtener mejores soluciones reduciendo el tamaño del paso,
a costa de tener que evaluar más veces la función $f$ numéricamente,
que es lo que más tiempo de computación necesita habitualmente.
Por tanto, el objetivo es
comparar los errores de truncamiento de diferentes métodos
conforme disminuye el tamaño del paso.

\begin{definition}
    Un \emph{método en diferencias a un paso} es un método que de paso fijo $h$
    que a partir de un estado $(t_i, \vw_i) \approx (t_i, \vy(t_i))$,
    aproxima $\vy(t_i + h)$ como
    \begin{equation*}
        w_{i+1} = w_i + h\cdot \vb*{\phi}(h, t_i, w_i).
    \end{equation*}
\end{definition}

\begin{definition}\label{def:elt}
    Sea $\vy$ la solución de un PVI.
    Se define \emph{el error local de truncamiento
    de un método en diferencias de paso $h$} como
    \begin{equation*}
        \ELT_{i+1}(h) =
        \frac{\vy(t_{i+1}) - \vy(t_i) - h\cdot \vb*{\phi}(h, t_i, \vy(t_i))}{h} =
        \frac{\vy(t_{i+1}) - \vy(t_i)}{h} - \vb*{\phi}(h, t_i, \vy(t_i)).
    \end{equation*}
\end{definition}

%TODO esta definición sería cogiendo la solución en el punto actual
% y la anterior es fijando una solución global asumiendo que el método
% no comete fallos
% \begin{definition}
%     Sean $f : \Omega \subset \R \times \R^n$ la función de un PVI
%     $p_i = (t_i, \vw_i)$ un estado del dominio de $f$,
%     $p_{i+1} = (t_i + h, \vw_{i+1})$
%     el siguiente estado generado por un método numérico que depende de $h$ e
%     $\vy_i$ una solución del PVI dada por $f$ y $p_i$.

%     Se define \emph{el error local de truncamiento del método} como
%     \begin{equation*}
%         Z_{i+1}(h) = \frac{\vy_i(t_i + h) - \vw_{i+1}}{h}.
%     \end{equation*}
%     En el caso de un método en diferencias,
%     \begin{equation*}
%         Z_{i+1}(h) =
%             \frac{\vy_i(t_i + h) - \vw_i}{h} - \vb*{\phi}(h, t_i, w_i).
%     \end{equation*}
% \end{definition}

%TODO revisar
\begin{example}
    En el caso concreto del método de Euler se tiene
    \begin{equation*}
        \ELT_{i+1}(h) =
        \frac{\vy(t_{i+1}) - \vy(t_i)}{h} - \vf(t_i,\vy(t_i)) =
        \frac{h}{2} \vy''(\xi_i),
            \qq{$\xi_i \in [t_i, t_{i+1}]$ para todo $i=0,\dots, n-1$}.
    \end{equation*}
    Y si $\norm{\vy''(t)} \le C$ para todo $t \in [a, b]$ entonces
    \begin{equation*}
        \norm{\ELT_{i+1}(h)} \le \frac{C}{2}h = \vo(h).
    \end{equation*}

    Lo que nos interesa es que este error (la norma)
    decrezca lo más rápido posible con $h$,
    es decir, que sea $\vo(h^p)$ con $p$ lo más grande posible.
    En esta búsqueda llegamos al siguiente método, el de Taylor.
\end{example}

\subsection{Notación para la diferenciación en varias variables}

A continuación en el método utilizamos la notación
$\pdv[2]{\vf(t_i, \vw_i)}{\vy}$
para representar la matriz hessiana que saldría al considerar $\vf$
como una función de $\vy$ exclusivamente.

Los lectores que prefieran razonar el método
cuando $\vy$ es un escalar en vez de un vector
pueden sustituir las evaluaciones de la diferencial y la hessiana
por multiplicaciones.
En el caso particular de la hessiana, saldría el término al cuadrado.

\subsection{El método de Taylor}
 
De aquí en adelante asumiremos el PVI general del principio,
y cualquier método que considere intentará aproximar una solución suya.

\begin{method}\label{met:taylor}
    \newcommand{\D}{\vb{D}}

    El \emph{método de Taylor de orden $p$} es un es un método de paso fijo $h$
    que a partir de un estado $(t_i, \vw_i) \approx (t_i, \vy(t_i))$,
    aproxima $\vy(t_i + h)$ como
    \begin{equation*}
        \vw_{i+1} =
        \vw_i + h\cdot \vb{T}^{(p)}(h, t_i, \vw_i),
    \end{equation*}
    donde
    \begin{align*}
        \vb{T}^{(p)} &=
            \vw_i + h\cdot \D^{(0)}(t_i, \vw_i)
            + \frac{h^2}{2}\D^{(1)}(t_i, \vw_i)
            + \cdots
            + \frac{h^p}{p!}\cdot \D^{(p-1)}(t_i, \vw_i) \\
        \D^{(0)}(t_i, \vw_i) &= \vf(t_i, \vw_i), \\
        \D^{(1)}(t_i, \vw_i) &=
            \pdv{\vf(t_i, \vw_i)}{t} +
            \pdv{\vf(t_i, \vw_i)}{\vy}\qty(\vf(t_i, \vw_i)), \\
        \D^{(2)}(t_i, \vw_i) &=
            \pdv[2]{\vf(t_i, \vw_i)}{t} +
            \pdv{\vf(t_i, \vw_i)}{t}{\vy}\qty(\D^{(1)}(t_i, \vw_i)) +
            \pdv[2]{\vf(t_i, \vw_i)}{\vy}\qty(
                \vf(t_i, \vw_i), \vf(t_i, \vw_i)
            ) \\
        \D^{(3)}(t_i, \vw_i) &= \cdots
    \end{align*}
\end{method}

\begin{remark}
    La expresión de $\vb{D}^{(i)}$ en el método de Taylor surge
    de de derivar la función $\vy'(t) = \vf(t, \vy(t))$ múltiples veces.
    De esa manera $\vy(t) + h\vb{T}^{(p)}(h, t, \vy(t))$ es el
    polinomio de Taylor de $\vy$.
\end{remark}

Unas pocas cuentas son suficientes para darse cuenta de que
calcular estas derivadas de manera genérica es realmente costoso,
y normalmente se suelen hacer las cuentas de manera \emph{adhoc}
para un problema concreto.

%TODO un ejemplillo aquí taría de puta madre

El siguiente teorema era de esperar.

\begin{theorem}
    Si $y$ es de clase $C^{(p+1)}([a, b])$,
    el ELT del método de Taylor de orden $p$ es $\vo(h^p)$.
\end{theorem}

\begin{proof}
    En estas condiciones el ELT no es más que el resto de Lagrange:
    \begin{equation*}
        \ELT_{i+1}(h) = \frac{h^p}{(p+1)!} \vf^{(p)}(\xi_i, y(\xi_i)),
            \qq{$\xi_i \in [t_i, t_{i+1}]$ para todo $i = 1,\ldots n$.}
    \end{equation*}
    Y si $\vf(t, \vy(t))$ está definida sobre un compacto
    entonces está acotada en $[a, b]$,
    y por lo tanto $\ELT_{i+1}(h) = \vo(h^p)$.
\end{proof}

A pesar de este gran resultado respecto al error,
la dificultad para aplicarlo de manera genérica y
los problemas que conlleva la derivación numérica no lo hacen muy práctico.
Buscamos entonces métodos que no necesiten estas derivadas
y que tengan órdenes similares.

\subsection{Métodos de Runge-Kutta}

\newcommand{\vbeta}{\vb*{\beta}}

Comenzamos buscando métodos de orden $2$,
para lo cual queremos aproximar
\begin{equation*}
    \vb{T}^{(2)}(h, t, \vy) =
    \vb{D}^{(0)}(t, \vy) + \frac{h}{2} \vb{D}^{(1)}(t, \vy) =
    \vf(t,y) + \frac{h}{2}\pdv{\vf(t, \vy)}{t} +
        \frac{h}{2}\pdv{\vf(t, \vy)}{\vy}\qty(\vf(t, \vy))
\end{equation*}
con un error de orden $\vo(h^2)$.
Una idea feliz es considerar una aproximación de la forma
$\vf(t + \alpha, \vy + \vbeta)$.
Aproximando esto último por Taylor tenemos
\begin{multline*}
    \vf(t + \alpha, y + \vbeta) = \\
    \qty(
        \vf(t,y) +
        \pdv{\vf(t, \vy)}{t} \cdot \alpha +
        \pdv{\vf(t, \vy)}{\vy} \qty(\vbeta) +
        \frac{1}{2}\pdv[2]{\vf(\xi, \mu)}{t} \cdot \alpha^2 +
        \pdv{\vf(\xi, \mu)}{t}{\vy} \qty(\vbeta) \cdot \alpha +
        \frac{1}{2}\pdv[2]{\vf(\xi, \mu)}{\vy} \qty(\vbeta, \vbeta)
    ).
\end{multline*}

Nótese que suponemos suficiente regularidad en $\vf$
para que las parciales cruzadas sean iguales.
Haciendo $a_1 = 1$, $\alpha = \frac{h}{2}$, $\vbeta = \frac{h}{2} \vf(t,y)$
la igualdad anterior se traduce en
\begin{multline*}
    f(t + \tfrac{h}{2}, y + \tfrac{h}{2}\vf(t, \vy)) = \\
    \vf(t,y) +
        \frac{h}{2}\pdv{\vf(t, \vy)}{t} +
        \frac{h}{2}\pdv{\vf(t, \vy)}{\vy} \qty(\vf(t, y)) +
        \frac{h^2}{8}\pdv[2]{\vf(\xi, \mu)}{t} +
        \frac{h^2}{4}\pdv{\vf(\xi, \mu)}{t}{\vy} \qty(\vf(t, y)) +
        \frac{h^2}{8}\pdv[2]{\vf(\xi, \mu)}{\vy} \qty(\vf(t, y), \vf(t, y)) = \\
    \vb{T}^{(2)}(t, y) + \frac{h^2}{8}\qty(
        \pdv[2]{\vf(\xi, \mu)}{t} +
        2\pdv{\vf(\xi, \mu)}{t}{\vy} \qty(\vf(t, y)) +
        \pdv[2]{\vf(\xi, \mu)}{\vy} \qty(\vf(t, y), \vf(t, y))
    ).
\end{multline*}

De nuevo bajo buenas condiciones de $\vf$
lo que hay entre paréntesis está acotado,
y al estar multiplicado por $h^2$ es de orden $\vo(h^2)$.
Hemos obtenido nuestro primer método de Runge-Kutta.

\begin{method}\label{met:mean-point}
    El \emph{método del punto medio} es un es un método de paso fijo $h$ que
    a partir de un estado $(t_i, \vw_i) \approx (t_i, \vy(t_i))$,
    aproxima $\vy(t_i + h)$ como
    \begin{equation*}
        \vw_{i+1} = \vw_i + h\cdot \vf\qty(
            t + \tfrac{h}{2}, \vy + \tfrac{h}{2}\vf(t, \vy)
        ).
    \end{equation*}
\end{method}

\begin{remark}
    Para este método solo necesitamos la función $\vf$, no sus derivadas,
    pero necesitamos dos evaluaciones en cada paso.
\end{remark}

\begin{remark}
    Al ser $\vf\qty(t + \tfrac{h}{2}, \vy + \tfrac{h}{2}\vf(t, \vy)) =
    \vb{T}^{(2)}(h, t, \vy) + \vo(h^2)$,
    el ELT será
    \begin{multline*}
        \ELT_{i+1}(h) =
        \frac{\vy(t_{i+1}) - \vy(t_i)}{h} - \qty(
            \vb{T}^{(2)}(h, t_i, \vy(t_i)) + \vo(h^2)
        ) =
        \qty(
            \frac{\vy(t_{i+1}) - \vy(t_i)}{h} - \vb{T}^{(2)}(h, t_i, \vy(t_i))
        ) - \vo(h^2) = \\
        \vo(h^2) - \vo(h^2) = \vo(h^2).
    \end{multline*}
\end{remark}

Podríamos intentar una estrategia parecida para $\vb{T}^{(3)}(t, \vy)$,
pero nuestros intentos fracasan porque
(haciendo las cuentas con escalares)
si intentamos aproximarlo mediante
\begin{equation}\label{eqn:t3-aproximation}
    T^{(3)}(t,y) =
    f(t,y) + \frac{h}{2}\dv{f(t, y(t))}{t}
        + \frac{h^2}{6}\dv[2]{f(t, y(t))}{t} \approx
    a_1f(t,y) + a_2f(t + \alpha, y + \delta f(t,y))
\end{equation}
pero ni siquiera esto basta para igualar el término
$\frac{h^2}{6}\qty[\pdv{f(t, y)}{y}]^2f(t,y)$
que resulta al desarrollar $\frac{h^2}{6}\dv[2]{f(t, y)}{t}$.
Invitamos al lector a hacer las cuentas.
Obtenemos sin embargo más métodos de orden $2$.

\begin{method}
    El \emph{método modificado de Euler} se obtiene de
    tomar $a_1 = a_2 = \frac{1}{2}$ y $\alpha = \delta = h$
    en la ecuación  \eqref{eqn:t3-aproximation}.
    \begin{equation*}
        \vw_{i+1} = \vw_i + \frac{h}{2}\qty\Big[
            \vf(t_i, \vw_i) + \vf(t_i + h, \vw_i + h\vf(t_i,\vw_i))
        ].
    \end{equation*}
\end{method}

\begin{method}
    El \emph{método de Heun} se obtiene de tomar $a_1 = \frac{1}{4}$,
    $a_2 = \frac{3}{4}$ y $\alpha = \delta = \frac{2}{3}h$
    en la ecuación \eqref{eqn:t3-aproximation}.
    \begin{equation*}
        \vw_{i+1} = \vw_i + \frac{h}{4}\qty\Big[
            \vf(t_i, \vw_i) +
            3\vf(t_i + \tfrac{2}{3}h, \vw_i + \tfrac{2}{3}h\vf(t_i, \vw_i))
        ].
    \end{equation*}

\end{method}

El método de Runge-Kutta más utilizado es el siguiente, de orden $4$.

\begin{method}\label{met:rk4}
    \newcommand{\K}{\vb{K}}

    El \emph{método de Runge-Kutta de orden $4$}
    es un es un método de paso fijo $h$ que
    a partir de un estado $(t_i, \vw_i) \approx (t_i, \vy(t_i))$,
    aproxima $\vy(t_i + h)$ como
    \begin{gather*}
        \vw_{i+1} = \vw_i + \frac{
            \K_1^{i+1} + 2\K_2^{i+1} + 2\K_3^{i+1} + \K_4^{i+1}
        }{6}. \\
        \begin{array}{ll}
        \K_1^{i+1} = h\vf\qty\bigg(t, \vw_i), &
        \K_2^{i+1} = h\vf\qty(t + \frac{h}{2}, \vw_i+ \frac{\K_1^{i+1}}{2}), \\
        \K_3^{i+1} = h\vf\qty(t + \frac{h}{2}, \vw_i+ \frac{\K_2^{i+1}}{2}), &
        \K_4^{i+1} = h\vf\qty\bigg(t + h, \vw_i + \K_3^{i+1}).
        \end{array}
    \end{gather*}
\end{method}

El lector puede haber empezado a intuir que cuanta más precisión queremos
más evaluaciones de $\vf$ necesitamos.
Una pregunta interesante es
el número mínimo de evaluaciones que necesitaríamos en cada paso
para conseguir una aproximación de cierto orden.
La \cref{tab:butcher} relaciona ambos conceptos.

\begin{table}[H]
    \centering
    \begin{tabular}{|l||l|l|l|l|}
        \hline
        Evaluaciones & $1\le p\le 4$ & $5\le p\le 7$ & $8\le p\le 9$
            & $10\le p$ \\
        \hline
        Mejor ELT & $\vo(h^p)$ & $\vo(h^{p-1})$ & $\vo(h^{p-2})$
            & $\vo(h^{p-3})$ \\
        \hline
    \end{tabular}
    \caption{Tabla de Butcher.
        Muestra, para un número $p$,
        el máximo orden de un método de Runge-Kutta con $p$ evaluaciones}
    \label{tab:butcher}
\end{table}

Para muchas aplicaciones,
se utiliza el número de evaluaciones de la función $\vf$
como una medida de la complejidad,
pues puede ser una función muy costosa.
Con nuestros avances actuales tenemos también dos formas de mejora
de la precisión de la solución numérica:
reducir el paso
y usar un método de orden mayor.
La \cref{tab:order-vs-step} muestra la comparación de estas dos formas
con los métodos de Runge-Kutta vistos hasta el momento,
fijando el número de evaluaciones a $40$
y suponiendo un intervalo con $b - a = 1$.
En general obtenemos mejores resultados aumentando el orden del método.

%TODO esto para que problema
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
    Método           & Paso  & Precisión aproximada \\
        \hline
        \hline
    RK4              & 0.1   & $10^{-4}$ \\
        \hline
    Euler Modificado & 0.05  & $2.5\cdot 10^{-3}$ \\
        \hline
    Euler            & 0.025 & $2.5\cdot 10^{-2}$ \\
        \hline
    \end{tabular}
    \caption{Tres métodos de Runge-Kutta;
        de órdenes $1$, $2$ y $4$;
        aplicados al mismo problema ajustando el paso
        para hacer el mismo número de evaluaciones de la función $\vf$.
    }
    \label{tab:order-vs-step}
\end{table}

