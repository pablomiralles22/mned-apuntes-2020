\section{Métodos de paso adaptativo}

\subsection{Control del error global a través del criterio del error local}

\begin{definition}
    Dado un problema de valor inicial y la solución
    $\qty{\vw_i}_{i = 0,\ldots,n}$
    calculada por un método numérico en diferencias,
    \begin{equation*}
        \left\{
        \begin{aligned}
            \vy'(t) &= \vf(t, \vy(t)) \\
            \vy(t_0) &= \vy_0 \\
        \end{aligned}
        \right.
        \qq{y}
        \left\{
        \begin{aligned}
            \vw_0 &= \vy_0 \\
            \vw_{i+1} &= \vw_i + h \vphi(h, t_i, \vw_i)
        \end{aligned},
        \right.
    \end{equation*}
    definimos la \emph{solución local} $\vz_i$
    como la solución del PVI
    \begin{equation*}
        \left\{
        \begin{aligned}
            \vz_i'(t) &= \vf(t, \vz_i) \\
            \vz_i(t_n) &= \vw_i
        \end{aligned}.
        \right.
    \end{equation*}
\end{definition}

Este último PVI es el que realmente aproximamos
en cada paso que damos en un método.
En la \cref{fig:local-sol} representamos
con una curva de color rojo la solución al problema original
y con curvas de color verde las soluciones locales.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[scale=1.5]
          \draw[->] (-0.5, 0) -- (3,0) node[right] {$t$};
          \draw[->] (0,-0.5) -- (0,2.5) node[above] {$y$};
          \draw [fill,red] (0.5,0.5) circle [radius=0.05] node[red,right] {$t_0$}; %primer punto del método
          \draw [fill, green] (1,1.3) circle [radius=0.05] node[green,right] {$t_1$}; %2º punto del método
          \draw [fill, green] (1.7,2) circle [radius=0.05] node[green,right] {$t_2$}; %3º punto del método
          
          \draw[red,-] (0.5,0.5)to[bend left] (3,1.5) ;
          \draw[green,-,dashed] (0.5,0.85)to[bend left] (3,1.8) ;
          \draw[green,-,dashed] (0.5,1.15)to[bend left] (3,2.05) ;
          \draw[green,-] (0.5,0.5) -- (1,1.3) ;
          \draw[green,-] (1,1.3) -- (1.7,2) ;
\end{tikzpicture}
    \caption{Una ilustración unidimensional del PVI que intenta resolver
    un método numérico en cada paso.
    En este caso las soluciones del PVI no varían bruscamente
    dada una pequeña pertubación en los datos de entrada.}
    \label{fig:local-sol}
\end{figure}

\begin{theorem}\label{thm:global-err}
    Supongamos que el método de integración verifica
    \begin{equation*}
        \norm{\vz_i(t_i + h) - \vw_{i+1}} \leq \varepsilon\cdot h
            \qq{(criterio de error local)}
    \end{equation*}
    para todo $i = 1,\ldots,n$ para una tolerancia $\varepsilon$ dada.
    Entonces, si $\vf$ es lipschitziana con constante $L$, se tiene que 
    \begin{equation*}
        \norm{\vy(t_n) - \vw_n} \leq \nume^{L(t_n - t_0)}\norm{\vy(t_n) - \vw_0}
            + \frac{\nume^{L(t_n - t_0)}}{L}\varepsilon.
    \end{equation*}
\end{theorem}

\begin{remark}
    Al lector le recordará a la cota que obtuvimos para el método de Euler
    (\cref{thm:euler-convergence}),
    solo que ésta es independiente del método.
    Además, si $L$ no es muy grande y $t_n - t_0 \approx 1$,
    tenemos una cota aceptable.
\end{remark}

\begin{remark}
    Observar que, si asumimos que $\vz_i(t_i) \approx \vy(t_i) \approx \vw_i $,
    este criterio consiste en mantener el ELT
    \begin{equation*}
        \norm{\ELT_{i+1}(h)} =
        \frac{\norm{\vy(t_{i+1}) - (\vy(t_i) + h\vphi(h, t_i, \vw_i))}}{h}
        \approx \frac{\norm{\vz_i(t_{i+1}) - \vw_{i+1}}}{h} \le \varepsilon
    \end{equation*}
    por debajo de la tolerancia.
\end{remark}

\subsection{Control del error por extrapolación de Richardson}

%TODO Este teorema debería tener demostración.
% En particular, parece que van a hacer falta condiciones sobre el PVI,
% para que z_i \approx z_{i+1}
\begin{theorem}
    \newcommand{\hh}{\sfrac{h}{2}}

    Supongamos que el método en diferencias 
    $\vw_{i+1} = \vw_i + h\cdot \vphi(h, t_i, \vw_i)$
    verifica
    \begin{equation*}\label{eq7}
        \vz_i(t_{i+1}) =
        \vw_{i+1} + h^{k+1}C\cdot \vz_i^{(k+1)}(t_i) + \vo(h^{k+2})
    \end{equation*}
    para una solución $\{(t_i, \vw_i)\}_{i = 0,\ldots,n}$ de paso $h$,
    donde $C$ es una constante que no depende de $h$.
    Entonces se tiene
    \begin{equation*}\label{eq8}
        \vz_i(t_{i+2}) =
        \vw_{i+2} + 2h^{k+1}C\cdot \vz_i^{(k+1)}(t_i) + \vo(h^{k+2}).
    \end{equation*}
\end{theorem}

\begin{remark}
    En las condiciones del teorema anterior el ELT es de orden $k$.
\end{remark}

\newcommand{\hh}{\sfrac{h}{2}}

Haciendo uso del teorema anterior, dadas dos soluciones
$\{(t^h_i, \vw^h_i)\}_{i = 0,\ldots,n}$ y
$\{(t^{\hh}_i, \vw^{\hh}_i)\}_{i = 0,\ldots,2n}$
calculadas con pasos $h$ y $\frac{h}{2}$,
\begin{align*}\label{eq8}
    \vz^{h}_i(t^{h}_{i+1}) &=
    \vw^{h}_{i+1} + h^{k+1}C\cdot (\vz^{h}_i)^{(k+1)}(t^{h}_i)
        + \vo(h^{k+2}), \\
    \vz^{\hh}_i(t^{\hh}_{i+2}) &=
    \vw^{\hh}_{i+2} + 2\qty(\frac{h}{2})^{k+1} \hspace{-4mm} C\cdot
        (\vz^{\hh}_i)^{(k+1)}(t^{\hh}_i) + \vo(h^{k+2}).
\end{align*}
Por tanto, si partiésemos del mismo punto
$(t^h_i, \vw^h_i) = (t^{\hh}_i, \vw^{\hh}_i)$,
sería $\vz^h_i(t^h_{i+1}) = \vz^{\hh}_i(t^{\hh}_{i+2})$
y podríamos obtener \emph{una fórmula para mejorar la solución},
\begin{equation*}
    \vz^h_i(t^h_{i+1}) = \vz^{\hh}_i(t^{\hh}_{i+2}) =
    \frac{2^k\vw^{\hh}_{i+2} - \vw^h_{i+1}}{2^k - 1} + \vo(h^{k+2}),
\end{equation*}
y \emph{una fórmula para aproximar el error local de truncamiento},
\begin{equation*}
    \vz^h_i(t^h_{i+1}) - \vw^h_{i+1} =
    \frac{2^k}{2^k-1}\qty(\vw^{\hh}_{i+2} - \vw^h_{i+1}) + \vo(h^{k+1}).
\end{equation*}

%TODO esto lo he quitado de momento.
% Se podría formatear y explicar mejor
% \begin{example}
% \begin{itemize}
%     \item Euler ($k=1$)  
%     $$
%     elt \approx 2\left(y_{\frac{h}{2}}(t_i+h)-y_h(t_i+h)\right)
%     $$
%     \item RK4 ($k=4$).
%     $$
%     elt \approx \frac{16}{15}\left(y_{\frac{h}{2}}(t_i+h)-y_h(t_i+h)\right)
%     $$
% \end{itemize}
% \end{example}

Este último avance es muy importante.
Hasta ahora habíamos podido estimar
la variación del error local de truncamiento en función de $h$,
que nos servía para comparar qué métodos aproximaban mejor
(suponiendo $h$ suficientemente pequeña)
y nos daba una cota del error global.
Pero el nuevo avance nos permite aproximar el error local.
Ahora proponemos un algoritmo que resuelve el problema
asegurándose de que el error local permanece acotado.

\begin{theorem}
    El siguiente procedimiento asegura que la aproximación final
    tiene ELT menor que $\varepsilon$.

    Parámetros:
    intervalo de resolución (\/$[a, b]$),
    cota para el error (\/$\varepsilon$),
    paso inicial (\/$h_0$).

    \begin{enumerate}
        \item Resolvemos el PVI dos veces en el intervalo $[a, b]$
        con tamaños de paso $h$ y $\frac{h}{2}$.
        \item Estimamos el máximo error local.
        \begin{equation*}
            \operatorname{err} = \max_{i = 1,\ldots,n}\qty{\ELT^h_i} \approx
            \max_{i = 1,\ldots,n}\qty{
                \frac{2^k}{2^k-1}\qty(\vw^{\hh}_{i+2} - \vw^h_{i+1})
            }.
        \end{equation*}
        \item Si el error no es menor que $\varepsilon\cdot h$,
        entonces dividimos $h$ entre $2$ y repetimos.
        \item Si el error estimado es menor o igual que $\varepsilon \cdot h$
        entonces el paso es el adecuado y
        devolvemos la extrapolación
        \begin{equation*}
            \vw_i := \frac{2^k\vw^{\hh}_{i+2} - \vw^h_{i+1}}{2^k - 1}.
        \end{equation*}
    \end{enumerate}
\end{theorem}

\begin{remark}
    El algoritmo anterior es un método de resolución
    que mantiene el error local local de truncamiento
    por debajo de $\varepsilon\cdot h$ (aproximadamente).
    Por tanto, el \cref{thm:global-err} asegura que el error global será
    menor o igual que $\varepsilon$ (también aproximadamente).
\end{remark}

La pregunta que vamos a responder ahora será la base de los métodos adaptativos.
Si tenemos una estimación del error en el paso anterior,
¿podemos estimar cuál tendría que haber sido el paso para tener un error menor?
Veámoslo.

Si escribimos el nuevo paso como $qh$, donde $q > 0$ sería el coeficiente corrector,
llegamos a la ecuación
\begin{align*}
    \norm{\vz_i(t_i + h) - \vw^{qh}_{i+1}} =
    \norm{(qh)^{k+1}C\cdot \vz_i^{(k+1)}(t_i) + \vo((qh)^{k+2})} = {} \\
    q^{k+1}\norm{h^{k+1}C\cdot \vz_i^{(k+1)}(t_i) + \vo(h^{k+2})} = {} \\
    q^{k+1}\norm{\vz_i(t_i + h) - \vw^{qh}_{i+1} + \vo(h^{k+2})} = {} \\
    q^{k+1}\norm{\ELT^h_{i+1} + \vo(h^{k+2})} \le {} & \varepsilon qh
\end{align*}
Y por tanto concluímos que una buena estimación para el nuevo paso
viene dada por
\begin{equation*}
    q \le \qty(\frac{\varepsilon h}{\ELT^h_{i+1}})^{\frac{1}{k}}.
\end{equation*}

\begin{remark}
    Obsérvese que $q < 1$ si y solo sí $\ELT^h_{i+1} > \varepsilon h$.
    Por tanto, la estimación del nuevo paso
    siempre reduce el paso cuando no tenemos la tolerancia esperada.
\end{remark}

Podemos entonces modificar el algoritmo anterior
para que converja más rápido.
No obstante, una idea más útil es
modificar el paso de nuestro método de forma dinámica
para mantener el error por debajo del umbral.
Es más, la misma ecuación que hemos planteado también nos sirve
para estimar cuánto se puede agrandar el paso
manteniendo la cota sobre el error de truncamiento local.


\subsection{Métodos adaptativos mediante extrapolación de Richardson}

\begin{definition}
    Un método es \emph{de paso adaptativo con tolerancia $\varepsilon$}
    si la solución $\{(t_i, \vw_i)\}_{i = 0,\ldots,n}$ que genera asegura
    \begin{equation*}
        \norm{\ELT_i} \le \varepsilon (t_i - t_{i-1})
            \qq{para todo $i = 1,\ldots,n$,}
    \end{equation*}
    a costa de modificar el tamaño del paso $h_i = t_i - t_{i-1}$
    en cada iteración.
\end{definition}

\begin{method}
    Fijado un método de paso fijo $M$ de orden $k$,
    su extrapolación adaptativa de Richardson
    es un método adaptativo de tolerancia $\varepsilon$.

    Una iteración, tal cual la definimos a continuación,
    no asegura que se genere un nuevo punto de la solución y
    utiliza un tamaño de paso $h$ que viene sugerido por la iteración anterior.
    Fijados $(t_i, \vw_i)$ y $h$,
    una iteración de este método
    calcula la aproximación $\vw^h$ dada por $M$ con un paso de tamaño $h$
    y $\vw^{\hh}$ dada por $M$ con dos pasos de tamaño $\frac{h}{2}$.
    Entonces, si el valor
    \begin{equation*}
        \operatorname{err} := \frac{2^k}{2^k-1}\norm{\vw^{\hh} - \vw^h}
    \end{equation*}
    verifica $\operatorname{err} < \varepsilon\cdot h$,
    determina el nuevo punto
    \begin{equation*}
        (t_{i+1}, \vw_{i+1}) = \qty(
            t_i + h, \frac{2^k\vw^{\hh}_{i+2} - \vw^h_{i+1}}{2^k - 1}
        ).
    \end{equation*}
    A continuación multiplica $h$ por
    $q = \qty(\frac{\varepsilon h}{\operatorname{err}})^{\frac{1}{k}}$,
    de cara a la siguiente iteración.
\end{method}

\begin{remark}
    El paso $h$ puede tanto aumentar como disminuir en cada iteración,
    y se modifica independientemente de que se acepte o no el punto.
\end{remark}

\begin{remark}
    Una implementación real del método de Richardson
    utiliza dos valores más,
    el máximo y el mínimo tamaño del paso.
    El máximo sirve para mantener el tamaño por debajo de un umbral
    independientemente de lo que sugiera el método,
    y el mínimo es para parar el método si fuese necesario reducir el paso
    por debajo de ese umbral,
    lo que impide que el método itere infinitamente por falta de convergencia.

    Además, empíricamente se ha visto que es mejor usar la expresión
    $q = \qty(\frac{\varepsilon h}{2\operatorname{err}})^{\frac{1}{k}}$
    porque aporta mejores resultados.
\end{remark}
    
\begin{remark}
    El coste de estimar el error de esta manera supone, aproximadamente,
    un incremento del $50\%$ en la cantidad de cómputo,
    comparándolo con solo calcular la extrapolación.
    Puede parecer un coste demasiado alto,
    pero la mayoría de problemas cuentan con
    zonas que necesitarían un tamaño de paso muy pequeño
    y zonas que bastaría con usar un tamaño de paso moderado.
    Por tanto, el ahorro de cómputo derivado de no tener que
    usar el mínimo paso requerido en todo el intervalo
    contrarresta en gran medida, en general,
    el incremento de cómputo.
\end{remark}  

\subsection{El método de Fehlberg}

\begin{editorial}
    Esta sección necesita todavía más detalle.
    Contribuye a estos apuntes via \href{https://git-scm.com/}{git}.
    Puedes encontrar el repositorio en
    \url{https://github.com/pablomiralles22/mned-apuntes-2020}.
\end{editorial}

Para ilustrar este método,
supogamos que tenemos dos métodos de aproximación.
El primero es un Runge-Kutta de orden $k$
\begin{equation*}
    \vw_{i+1} = \vw_i + h\vphi(h, t_i, \vw_i) + \vo(h^{k+1})
\end{equation*}
que produce las aproximaciones
\begin{equation*}
    \left\{
    \begin{array}{lll}
    \vw_0 = \alpha & & \\
    \vw_{i+1} = \vw_i + h\phi(t_i, \vw_i, h) & & i >0
    \end{array}
    \right.
\end{equation*}
con error de truncamiento $\ELT_{i+1}(h) = \vo(h^k)$,
y el segundo es otro método de Runge-Kutta pero de orden $k+1$
\begin{equation*}
    \bar\vw_{i+1} = \bar\vw_i + h\bar\vphi(h, t_i, \bar\vw_i) + \vo(h^{k+2})
\end{equation*}
que produce las aproximaciones
\begin{equation*}
    \left\{
    \begin{array}{lll}
    \tilde{\vw_0} = a & & \\
    \bar\vw_{i+1} = \bar\vw_i + h\tilde{\phi}(t_i, \bar\vw_i, h) & & i >0
    \end{array}
    \right.
\end{equation*}
con error de truncamiento $\ELT_{i+1}(h) = \vo(h^{k+1})$.

Suponiendo $\vw_i = \bar\vw_i \approx y(t_i)$ y tomando un $h$ fijo tenemos que
\begin{equation*}
\begin{array}{l}
    \ELT_{i+1}(h) = \frac{y(t_{i+1}) - y(t_i)}{h} - \phi(t_i, y(t_i), h) \approx  \frac{y(t_{i+1}) - \vw_i}{h} - \phi(t_i, \vw_i, h) =\\
    \\
     =\frac{y(t_{i+1}) - (\vw_i+ - h\phi(t_i, \vw_i, h))}{h} = \frac{1}{h}(y(t+i)-\vw_{i+1})
\end{array}
\end{equation*}
Y, análogamente, se tiene que 
\begin{equation*}
    \bar\ELT_{i+1}(h) \approx \frac{1}{h}(y(t+i)-\bar\vw_{i+1})
\end{equation*}
Si ahora sumamos y restamos el término $\frac{1}{h}\bar\vw_{i+1}$
en la aproximación obtenida para $\ELT_{i+1}(h)$,
llegamos a que
\begin{equation*}
    \ELT_{i+1}(h) = \frac{1}{h}[(y(t_{i+1})-\bar\vw_{i+1}) + (\bar\vw_{i+1} - \vw_{i+1})] = \bar\ELT_{i+1}(h) + \frac{1}{h}(\bar\vw_{i+1}-\vw_{i+1}).
\end{equation*}
Ahora bien, como $\bar\ELT_{i+1}(h)$ es $\vo(h^{k+1})$,
podemos aproximar el ELT del método de orden $k$ como
\begin{equation*}
\ELT_{i+1}(h) \approx \frac{1}{h}(\bar\vw_{i+1}-\vw_{i+1}),
\end{equation*}
de manera que la aproximación del error local de truncamiento
al tomar el paso de tamaño $qh$ es
\begin{equation*}
    \ELT_{i+1}(qh)\approx C(qh)^k = Cq^nh^k \approx q^k \ELT_{i+1}(h) \approx \frac{q^k}{h}(\bar\vw_{i+1}-\vw_{i+1}),
\end{equation*}
donde hemos utilizado que $\ELT_{i+1}(h) \approx Ch^k$
por ser $\ELT_{i+1}(h)$ una $\vo(h^k)$.

Para establecer la cota del error local de truncamiento
por la tolerancia $\varepsilon$ tendríamos, por tanto,
que escoger $q$ tal que
\begin{equation*}
    q \leq \left(\frac{\varepsilon h}{|\bar\vw_{i+1} - \vw_{i+1}|}\right)^{\frac{1}{k}}
\end{equation*}
Un método muy usado que utiliza esta última desigualdad para
controlar el error local de truncamiento es
\emph{el método de Runge-Kutta-Felhberg},
que pasamos a introducir a continuación.

Si eligiésemos dos métodos arbitrarios de Runge-Kutta de cuarto y quinto orden
para aplicar lo que acabamos de ver,
necesitaríamos, como mínimo, $10$ evaluaciones de la función $\vf$
($4$ por el método de orden $4$ y $6$ por el de orden $5$).
Sin embargo, Erwin Fehlberg encontró $2$ métodos de Runge-Kutta de estos órdenes
que se pueden anidar de manera que
se necesiten tan solo $6$ evaluaciones de $\vf$ para calcular ambos.
El \textbf{método de Runge-Kutta-Fehlberg} consiste, por tanto, en
emplear el método de Runge-Kutta
\begin{equation*}
    \bar\vw_{i+1} = \vw_i + \frac{16}{135}\vb{K}_1 + \frac{6656}{12825}\vb{K}_3 + \frac{28561}{56430}\vb{K}_4 - \frac{9}{50}\vb{K}_5 + \frac{2}{55}\vb{K}_6,
\end{equation*}
con un ELT de quinto orden para estimar el ELT
en el método de Runge-Kutta de cuarto orden dado por 
\begin{equation*}
    \vw_{i+1} = \vw_i + \frac{25}{216}\vb{K}_1 + \frac{1408}{2565}\vb{K}_3 + \frac{2197}{4104}\vb{K}_4 - \frac{1}{5}\vb{K}_5,
\end{equation*}
donde 
\begin{equation*}
\begin{array}{l}
    \vb{K}_1 =  hf\left(t_i, \vw_i\right)\\
    \vb{K}_2 =  hf\left(t_i + \frac{h}{4}, \vw_i + \frac{1}{4}\vb{K}_1\right)\\
    \vb{K}_3 =  hf\left(t_i + \frac{3h}{8}, \vw_i + \frac{3}{32}\vb{K}_1 +  \frac{9}{32}\vb{K}_2\right)\\
    \vb{K}_4 = hf\left(t_i + \frac{12h}{13}, \vw_i + \frac{1932}{2197}\vb{K}_1 -   \frac{7200}{2197}\vb{K}_2 +  \frac{7296}{2197}\vb{K}_3\right) \\
    \vb{K}_5 = hf\left(t_i + h, \vw_i + \frac{439}{216}\vb{K}_1 - 8k_2 +  \frac{3680}{513}\vb{K}_3 - \frac{845}{4104}\vb{K}_4\right) \\
    \vb{K}_6 = hf\left(t_i + \frac{h}{2}, \vw_i - \frac{8}{27}\vb{K}_1 + 2k_2 -  \frac{3544}{2565}\vb{K}_3 + \frac{1859}{4104}\vb{K}_4 - \frac{11}{40}\vb{K}_5\right)
\end{array}
\end{equation*}
