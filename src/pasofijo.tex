\section{Preliminares}

%TODO cambiar esta label y actualizar referencias
\begin{definition}\label{eqn:pvi}
    Un \emph{problema de valor inicial} (en adelante, PVI)
    es una ecuación diferencial ordinaria dada por
    $f : \Omega \subset \R \times \R^n \to \R^n$
    acompañada de una condición inicial $(t_0, \vy_0)$ del dominio de $f$
    del que se busca una solución $y$ que cumpla
    \begin{equation*}
        \left\{
        \begin{aligned}
            \vy'(t) &= f(t, \vy(t)) \\
            \vy(t_0) &= \vy_0 \\
        \end{aligned}.
        \right.
    \end{equation*}
\end{definition}

%TODO la definición no deja cuestionarse el orden, no?
% \begin{remark}
%     Los problemas que trataremos serán de orden $1$ siempre,
%     en otro caso los transformaremos en un problema de orden $1$.
% \end{remark}

A lo largo del texto hablaremos de la solución $\vy$ de un PVI,
por lo que estaremos asumiendo que existe una solución y que es única.
Esto será así en la mayoría de problemas que queramos resolver,
a poco que la función $f$ del PVI cumpla unos requisitos mínimos.

\begin{theorem}[Picard-Lindelöf]
    Sea $f(t, \vy) : \Omega \subset \R \times \R^n \to \R^n$,
    donde $\Omega$ es abierto,
    una función continua y
    localmente Lipschitziana respecto de la segunda variable.
    Entonces, dado $(t_0, \vy_0) \in \Omega$,
    existe un intervalo cerrado $[t_0 - h, t_0 + h]$
    donde existe una única solución del PVI
    \begin{equation*}
        \left\{
        \begin{aligned}
            \vy'(t) &= f(t, \vy(t)) \\
            \vy(t_0) &= \vy_0 \\
        \end{aligned}.
        \right.
    \end{equation*}
    que cumple que $(t, \vy(t)) \in \Omega$ para todo $t \in [t_0 - h, t_0 + h]$.
\end{theorem}

\begin{proof}
    La prueba de este teorema corresponde a
    un curso previo de ecuaciones diferenciales.
\end{proof}

\begin{remark}
    La condición de que $f$ sea Lipschitziana no es necesariamente
    muy restrictiva.
    Pensemos que es suficiente con que $f$ está definida sobre un convexo y
    que tenga derivada con respecto a la segunda variable continua y acotada.
    Lo que también es cierto si ese convexo es compacto.
\end{remark}

\subsection{Notación}

%TODO habría que considerar que t_i también se aproxima por tilde{t}_i, no?
Los métodos que veremos calcularán soluciones de un PVI
como una lista de pares $\{(t_0, \vw_0), \ldots, (t_n, \vw_n)\}$,
donde los $t_i$ son reales de forma que $t_0 < t_1 < \cdots < t_n$
y cada punto $\vw_i$ es una aproximación de $\vy(t_i)$,
$\vw_i \approx \vy(t_i)$.
No obstante, una implementación utilizaría números de precisión finita,
produciendo una lista $\{t_0, \tw_0), \ldots, (t_n, \tw_n)\}$,
de forma que
\begin{equation*}
    \tw_i \approx \vw_i \approx \vy(t_i).
\end{equation*}

\section{Métodos de paso fijo}

\begin{definition}
    Un método es \emph{de paso fijo $h$} si la solución
    $\{(t_i, \vw_i)\}_{i = 0,\ldots,n}$ que genera cumple
    \begin{equation*}
        t_i = t_{i-1} + h \qq{para todo $i = 1,\ldots,n$.}
    \end{equation*}
\end{definition}

Cuando resolvamos un PVI, generaremos una cantidad finita de puntos.
A menudo, nos interesará conocer la solución en un intervalo $[a, b]$,
$t_0 = a$,
para lo cual lo normal es generar puntos hasta que $t_n > b$.
En general, asumiremos que $t_n = b$.
O mejor dicho, el lector puede entender que $a$ representa $t_0$
y $b$ es $t_n$ para la solución concreta que estemos tratando.

\subsection{El método de Euler}

\begin{method}\label{met:euler}
    El \emph{método de Euler} es un es un método de paso fijo $h$ que
    a partir de un estado $(t_i, \vw_i) \approx (t_i, \vy(t_i))$,
    aproxima $\vy(t_i + h)$ como
    \begin{equation*}
        w_{i+1} = w_i + h\cdot f(t_i, w_i).
    \end{equation*}
\end{method}

A continuación presentamos una desigualdad elemental que utilizaremos
en la estimación del error del método de Euler.

\begin{lemma}\label{lema2}
    Se cumplen las siguientes desigualdades:
    \begin{enumerate}
        \item $1 + x \le e^x$ para todo $x \in \mathbb{R}$.
        \item $0 \le (1+x)^m \le e^{mx}$ para todo $m > 0$ y $x \ge -1$.
    \end{enumerate} 
\end{lemma}

\begin{proof}
    Es claro que la segunda desigualdad es consecuencia de la primera.
    La primera tiene una demostración geométrica sencilla,
    puesto que $1 + x$ es la línea tangente en $x = 0$ a $y = \nume^x$,
    y las tangentes se encuentran por debajo de una función convexa.
    Resultado que se puede probar usando un desarrollo de Taylor.
    \begin{equation*}
        \nume^x = 1 + x + \frac{x^2}{2}\nume^\xi \qq{con $\abs{\xi} < \abs{t}.$}
        \implies \nume^x \ge 1+ x.
    \end{equation*}
    Donde hemos usado que $\frac{x^2}{2}\nume^\xi \ge 0$.
\end{proof}

%TODO follow here Emilio
\begin{theorem}[Convergencia del método de Euler]
    Sea $f:D\subset \mathbb{R}^2\rightarrow \mathbb{R}$ con $D$ abierto,
    e $Y(t)$ una solución de $(1)$ con $(t,Y(t))\in D,\forall t\in[a,b]$.
    Sea también la solución \ref{eqn:eulersol} para cierto $h$ fijo.
    Si se cumple
    \begin{enumerate}[label=(\alph*)]
        \item f Lipschitziana respecto de la segunda variable en $D$, con constante de Lipschitz $K$.
        \item $Y''$ existe en todo $[a,b]$ y está acotada por una constante $C\ge 0$.
        \item $(t_i,w_i)\in D,\forall i=0,\dots,n$.
    \end{enumerate}
    entonces
    \begin{equation*}
        \max_{0\le i \le n}|Y(t_i)-w_i| \le e^{(b-a)K}\cdot |Y(a)-w_0| + \frac{e^{(b-a)K}-1}{2K}ch
    \end{equation*}
\end{theorem}

\begin{proof}
    \begin{equation}
\label{eq1}
\left\{
\begin{array}{l}
Y(t_{n+1}) = Y(t_n) + hY'(t_n) + \frac{1}{2}h^2Y''(\xi) = Y(t_n) + hf(t_n, Y(t_n)) + \frac{1}{2}h^2Y''(\xi)\\
\\
y_h(t_{n+1}) = y_h(t_n) + hf(t_n, y_h(t_n))
\end{array}
\right. 
\end{equation}
\begin{equation}
\label{eq2}
\implies Y(t_{n+1}) - y_h(t_{n+1}) = (Y(t_n) - y_h(t_n)) + h(f(t_n, Y(t_n)) - f(t_n, y_h(t_n))) + \frac{1}{2}h^2Y''(\xi)
\end{equation}
De esta última expresión se deduce, llamando $error_i = \|Y(t_i) - y_h(t_i)\|$ para $0\le i \le N$ y teniendo en cuenta que $\left\|\frac{1}{2}h^2Y''(\xi)\right\| \le \frac{1}{2}h^2c$:
$$
error_{n+1} = \|Y(t_{n+1}) - y_h(t_{n+1})\| \le \|Y(t_n) - y_h(t_n)\| + h\overset{\le k\| Y(t_n) - y_h(t_n)\|}{\overbrace{\|f(t_n, Y(t_n)) - f(t_n, y_h(t_n))\|}}  + \frac{1}{2}h^2c \le
$$
$$
\le (1+hk)\|Y(t_n) - y_h(t_n)\| + \frac{1}{2}h^2c = (1+hk)error_n + \frac{1}{2}h^2c
$$
Es decir, se obtiene que $error_{n+1} \le (1+hk)error_n + \frac{1}{2}h^2c$.\\\\Si ahora utilizamos esta última expresión recursivamente como sigue
$$
error_{n+1} \le (1+hk)error_n + \frac{1}{2}h^2c \le (1+hk)\left[(1+hk)error_{n-1} + \frac{1}{2}h^2c\right] + \frac{1}{2}h^2c
$$
Llegamos a que
$$
error_{n+1} \le (1+hk)^{n+1}error_0 + \overset{\textnormal{serie geométrica entre 0 y n}}{\overbrace{\left[ 1 + (1+hk) + (1+hk)^2 + \cdots + (1+hk)^{n}\right]}}\frac{1}{2}h^2c =  
$$
$$
= (1+hk)^{n+1}error_0 + \left[\frac{(1+hk)^{n+1} - 1}{hk}\right]\frac{1}{2}h^2c = (1+hk)^{n+1}error_0 + \left[\frac{(1+hk)^{n+1} - 1}{k}\right]\frac{1}{2}hc \overset{\ref{lema2}}{\le} 
$$
$$
\le e^{hk(n+1)}error_0 + \left[\frac{e^{hk(n+1)} - 1}{k}\right]\frac{1}{2}hc =  e^{(b-a)k}\|Y(t_0) - y_h(t_0)\| + \left[\frac{e^{(b-a)k} - 1}{k}\right]\frac{1}{2}hc \qedhere
$$
\end{proof}

\begin{theorem}[Convergencia del método de Euler con error de redondeo]
    Sea $f:D\subset \mathbb{R}^2\rightarrow \mathbb{R}$ con $D$ abierto, e $Y(t)$ una solución de $(1)$ con $(t,Y(t))\in D,\forall t\in[a,b]$.

    Fijado $h>0$, sea la solución numérica

\begin{equation}
\begin{cases}
    w_0=y_0 \\
    w_{i+1}=w_i + h\cdot f(t_i, w_i) + \delta_i
\end{cases}
\end{equation}

con $\delta_i<\delta,\forall i=0,\dots,n$.

    Si se cumple
    \begin{enumerate}[label=(\alph*)]
        \item f Lipschitziana respecto de la segunda variable en $D$, con constante de Lipschitz $K$.
        \item $Y''$ existe en todo $[a,b]$ y está acotada por una constante $C\ge 0$.
        \item $(t_i,w_i)\in D,\forall i=0,\dots,n$.
    \end{enumerate}
    entonces $$\max_{0\le i \le n}|Y(t_i)-w_i| \le e^{(b-a)K}\cdot |Y(a)-w_0| + \frac{e^{(b-a)K}-1}{K}\left(\frac{1}{2}ch+\frac{\delta}{h}\right)$$

\end{theorem}

Se puede estimar el error de otra forma bajo ciertas condiciones, que nos será muy útil para estimar numéricamente el error e incluse obtener soluciones mejores a partir del método de Euler.

\begin{theorem}
    Dado el PVI \ref{eqn:pvi} y la solución \ref{eqn:eulersol}, si la solución $Y(t)$ real es de clase $C^3$ y $\frac{\partial f}{\partial y},\frac{\partial^2f}{\partial y^2}$ constantes, entonces
    \begin{equation}
    Y(t_i)-w_i=h\cdot D(t_i)+\theta(h^2),\forall t\in[t_0,b]
    \end{equation}
    dónde $D$ es solución del siguiente PVI:
\begin{equation}
\begin{cases}
    d'(t)=g(t)d(t) + \frac{1}{2} Y''(t), g(t)=\frac{\partial f(t,y)}{\partial y}(t,Y(t)) \\
    d(a) = 0 \\
    t\in[a,b]
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
Puede encontrarse en el libro de Atkinson, pero no entra.
\end{proof}

\begin{remark}
Cuando $D(t)$ se puede obtener explícitamente, el término $hD(t_i)$ de la fórmula  (\ref{eq3}) normalmente proporciona una estimación bastante buena del error real cometido ($Y(t_i) - y_h(t_i)$), y la precisión de la estimación mejora si se disminuye el tamaño del paso $h$.
\end{remark}
 No suele ser muy práctico encontrar $D$, pero el resultado nos ayuda a demostrar lo siguiente:

\begin{theorem}
    En condiciones similares a las del teorema anterior, tomando $(t_i^h,w_i^h)$ y $(t_i^{h/2},w_i^{h/2})$ soluciones obtenidas con el método de Euler con pasos $h$ y $\frac{h}{2}$ respectivamente:
    \begin{enumerate}[label=(\alph*)]
        \item $Y(t_i^h)-w_{2i}^{h/2} = (w_{2i}^{h/2}-w_i^h) + \theta(h^2)$.
        \item $w_i := 2\cdot w_{2i}^{h/2}-w_i^h$ es una aproximación de $Y(t)$ de orden $\theta(h^2)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Aplicando el teorema anterior obtenemos:
    $$
    Y(t_i^h)-w_i^h=h\cdot D(t_i^h)+\theta(h^2),\forall t\in[t_0,b]
    $$
    $$
    Y(t_{2i}^{h/2})-w_{2i}^{h/2}=h\cdot D(t_{2i}^{h/2})+\theta(h^2),\forall t\in[t_0,b]
    $$
    Basta observar que $t_i^h = t_{2i}^{h/2}$ y restar la primera ecuación a la segunda multiplicada por 2. Despejando se obtiene tanto (a) como (b).

\end{proof}

\subsection{Estimación general del error}

Como hemos visto, a partir de conocer el error cometido en cada paso hemos logrado estimar globalmente el error de la solución generada por el método de Euler. La idea es hacer esto de manera general, pero de momento nos conformamos con formalizar la idea de error local.

\begin{definition}
Dado un método de diferencias
\begin{equation} \label{eqn:diffmet}
\begin{cases}
    w_0=\alpha \\
    w_{i+1}=w_i + h\cdot \phi(t_i, w_i), \forall i=0,\dots, n-1
\end{cases}
\end{equation}
    que devuelva una solución de \ref{eqn:pvi} con solución real $Y$, se define su \textbf{error local de truncamiento (e.l.t.)} como
    $$
    T_{i+1}(h) = \frac{Y(t_{i+1})-Y(t_i)-h\phi(t_i,Y(t_i))}{h}
    = \frac{Y(t_{i+1})-Y(t_i)}{h} -h\phi(t_i,Y(t_i))
    $$
\end{definition}


\begin{example}
En el caso concreto del método de Euler se tiene:
    $$
    T_{i+1}(h)
    = \frac{Y(t_{i+1})-Y(t_i)}{h} -h\cdot f(t_i,Y(t_i))= \frac{h}{2} Y''(\xi_i),
    \xi_i\in[t_i,t_{i+1}], \forall i=0,\dots, n-1
    $$

    y si $|Y''(t)|\le C,\forall t\in[a,b]$ entonces
    $$
    |T_{i+1}(h)|\le \frac{C}{2} \cdot h
    $$
    que es $\theta(h)$.

Lo que nos interesa es que este error decrezca lo más rápido posible con h, es decir, que sea $\theta(h^p)$ con $p$ lo más grande posible. En esta búsqueda llegamos al siguiente método, el de Taylor.
\end{example}

\subsection{Método de Taylor}
 
De aquí en adelante asumiré el PVI general del principio, y cualquier método que considere intentará aproximar una solución suya.

\begin{definition}
Se define el método de Taylor de orden p como:
\begin{equation}
\begin{cases}
    w_0=\alpha \\
    w_{i+1}=w_i + h\cdot T^{(p)}(t_i, w_i), \forall i=0,\dots, n-1
\end{cases}
\end{equation}

    dónde $T^{(p)}(t_i,w_i)=\sum_{i=0}^{p-1} \frac{h^i}{(i+1)!}f^{(i)}(t_i,w_i)$, y $f^{(i)}(t_i,w_i)$ viene dado po $Y^{(i+1)}(t)=(f(t,Y(t)))^{(i)}$.
\end{definition}

Unas pocas cuentas son suficientes para darse cuenta de que calcular estas derivadas de manera genérica es realmente costoso, y normalmente se suelen hace las cuentas de manera \textit{adhoc} para un problema concreto.

El siguiente teorema era de esperar:

\begin{theorem}
    Si Y es de clase $C^{(p+1)}([a,b])$, el e.l.t. del método de Taylor de orden p es $\theta(h^p)$.
\end{theorem}
\begin{proof}
En estas condiciones el e.l.t. no es más que el resto de Lagrange:

$$
    T_{i+1}(h)=\frac{h^p}{(p+1)!} f^{(p)}(\xi_i,Y(\xi_i)),\xi_i\in [t_i,t_{i+1}]\subset[a,b]
$$
    Por ser $f(t,Y(t))$ contínua en compacto está acotada en $[a,b]$, y eso es por lo tanto $\theta(h^p)$.
\end{proof}

%TODO esto solo para una dimensión?

A pesar de este gran resultado respecto al error, la dificultad para aplicarlo de manera genérica y los problemas que conlleva la derivación numérica no lo hace muy práctico. Buscamos entonces métodos que no necesiten estas derivadas y que tengan órdenes similares.

\subsection{Métodos de Runge-Kutta}

Comenzamos buscando métodos de orden 2, para lo cual queremos aproximar $T^{(2)}(t,y)=f(t,y)+\frac{h}{2} f'(t,y)$ con un error de orden $\theta(h^2)$. Una idea feliz es considerar una aproximación de la forma $a_1f(t+\alpha_1, y+\beta_1)$. Aproximando esto último por Taylor tenemos:

$$
a_1f(t+\alpha_1, y+\beta_1)=a_1\left(
f(t,y)+
\frac{\partial f}{\partial t}(t,y)\cdot \alpha_1+
\frac{\partial f}{\partial y}(t,y)\cdot \beta_1+
\frac{\partial^2 f}{\partial t^2}(\xi,\mu)\cdot \frac{\alpha_1^2}{2}+
\frac{\partial^2 f}{\partial t \partial y}(\xi,\mu)\cdot \alpha_1\beta_1+
\frac{\partial^2 f}{\partial y^2}(\xi,\mu)\cdot \frac{\beta_1^2}{2}
\right)
$$

Nótese que suponemos suficiente regularidad en f para que las parciales cruzadas sean iguales. Haciendo $a_1=1,\alpha_1=\frac{h}{2}, \beta_1=\frac{h}{2} f(t,y)$ la igualdad anterior se traduce en:

$$
f(t+\frac{h}{2}, y+\beta_1)=
f(t,y)+
\frac{\partial f}{\partial t}(t,y)\cdot \frac{h}{2}+
\frac{\partial f}{\partial y}(t,y)\cdot \frac{h}{2} f(t,y)+
\frac{\partial^2 f}{\partial t^2}(\xi,\mu)\cdot \frac{h^2}{8}+
\frac{\partial^2 f}{\partial t \partial y}(\xi,\mu)\cdot \frac{h^2}{4}f(t,y)+
\frac{\partial^2 f}{\partial y^2}(\xi,\mu)\cdot \frac{h^2}{8}f(t,y)
$$

$$
=T^{(2)}(t,y)+
h^2 \left(
\frac{\partial^2 f}{\partial t^2}(\xi,\mu)\cdot \frac{1}{8}+
\frac{\partial^2 f}{\partial t \partial y}(\xi,\mu)\cdot \frac{1}{4}f(t,y)+
\frac{\partial^2 f}{\partial y^2}(\xi,\mu)\cdot \frac{1}{8}f(t,y)
\right)
$$

De nuevo bajo buenas condiciones de $f$ lo que hay entre paréntesis está acotado, y al multiplicar por $h^2$ tenemos el orden de error buscado. Esto nos da nuestro primer método de Runge-Kutta:

\begin{definition}[Método del Punto Medio]
\begin{equation}
\begin{cases}
    w_0=\alpha \\
    w_{i+1}=w_i + h\cdot f(t+\frac{h}{2}, y+\frac{h}{2}f(t,y)) , \forall i=0,\dots, n-1
\end{cases}
\end{equation}
\end{definition}

\begin{remark}
    En este método no necesitamos solo necesitamos la función $f$, pero necesitamos dos evaluaciones en cada paso, que puede ser costoso.
\end{remark}

\begin{remark}
    Al ser $ f(t+\frac{h}{2}, y+\beta_1)=T^{(2)}(t,y)+\theta(h^2) $, el e.l.t. será:
    $$
    T_{i+1}(h)= \left(\frac{Y(t_{i+1})-Y(t_i)-h\phi(t_i,Y(t_i))}{h} - T^{(2)}(t_i, y(t_i))\right) - \theta(h^2)
    =\theta(h^2)- \theta(h^2)
    =\theta(h^2)
    $$
\end{remark}

Podríamos intentar una estrategia parecida para $T^{(3)}(t,y)$:

\begin{equation}
\label{eq5}
T^{(3)}(t,y) = f(t,y) + \frac{h}{2}f'(t,y) + \frac{h^2}{6}f''(t,y) \approx a_1f(t,y) + a_2f(t+\alpha_2, y+\delta_2f(t,y))
\end{equation}
pero ni siquiera esto basta para igualar el término $ \frac{h^2}{6}\left[ \frac{\partial f}{\partial y}(t,y)\right]^2f(t,y)$, que resulta al desarrollar $\frac{h^2}{6}f''(t,y)$.
Obtenemos sin embargo más métodos de orden 2:

\begin{definition}[Método modificado de Euler]
Se obtiene al tomar $a_1 = a_2 = \frac{1}{2}$ y $\alpha_2 = \delta_2 = h$ en la ecuación  (\ref{eq5}).
\begin{equation}
\begin{cases}
    w_0=\alpha \\
    w_{i+1}=w_i + \frac{h}{2}\left[f(t_i, w_i) + f(t_{i+1}, w_i+hf(t_i,w_i))\right]  , \forall i=0,\dots, n-1
\end{cases}
\end{equation}

\end{definition}

\begin{definition}[Método de Heun]
Este método se obtiene al tomar $a_1 = \frac{1}{4}$, $a_2 = \frac{3}{4}$ y $\alpha_2 = \delta_2 = \frac{2}{3}h$ en la ecuación  (\ref{eq5}).
\begin{equation}
\begin{cases}
    w_0=\alpha \\
    w_{i+1}=w_i + \frac{h}{4}\left[f(t_i, w_i) + 3f(t_i + \frac{2}{3}h, w_i+\frac{2}{3}hf(t_i,w_i))\right] , \forall i=0,\dots, n-1
\end{cases}
\end{equation}

\end{definition}

El método más utilizado de Runge-Kutta es el siguiente, de orden 4:

\begin{definition}[Método de Runge-Kutta de orden 4]
\begin{equation}
\begin{cases}
    w_0=\alpha \\
    K_1^{i+1} = hf(t,w_i)\\
    K_2^{i+1} = hf(t + h/2,w_i+ K_1^{i+1}/2)\\
    K_3^{i+1} = hf(t + h/2,w_i+ K_2^{i+1}/2)\\
    K_4^{i+1} = hf(t + h,w_i + K_3^{i+1})\\

    w_{i+1}=w + \frac{K_1^{i+1} + 2K_2^{i+1} + 2K_3^{i+1} + K_4^{i+1}}{6}
\end{cases}
\end{equation}

\end{definition}


Es fácil ver que si queremos más precisión tenemos que utilizar más evaluaciones de $f$. Una pregunta interesante es el número mínimos de evaluaciones en cada paso para conseguir una aproximación de cierto orden. La siguiente tabla relaciona ambos conceptos:

\begin{table}[H]
\centering
\begin{tabular}{|l||l|l|l|l|}
    \hline
Evaluaciones & $1\le p \le 4$ & $5\le p\le 7$ & $8\le p\le 9$  & $10\le p$ \\
    \hline
Mejor e.l.t. & $\theta(h^p)$ & $\theta(h^{p-1})$ & $\theta(h^{p-2})$ & $\theta(h^{p-3})$ \\
    \hline
\end{tabular}
\caption{Tabla de Butcher de evaluaciones y órdenes del e.l.t.}
\end{table}

Muchas veces utilizamos el número de evaluaciones de la función $f$ como medida de complejidad, pues puede se muy costosa.

Tenemos también dos formas de mejora la precisión de la solución numérica: reducir el paso y usar un método de orden mayor. Veamos la comparación de estas dos formas con los métodos de Runge-Kutta vistos hasta el momento, fijando el número de evaluaciones a $40$ y suponiendo un intervalo con $b-a=1$:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
    \hline
Método           & Paso  & Precisión aproximada \\
    \hline
    \hline
RK4              & 0.1   & $10^{-4}$ \\
    \hline
Euler Modificado & 0.05  & $2.5\cdot 10^{-3}$ \\
    \hline
Euler            & 0.025 & $2.5\cdot 10^{-2}$ \\
    \hline
\end{tabular}
\end{table}

En general obtenemos mejores resultados aumentando el orden del método.


\section{Métodos de paso adaptativo}
\subsection{Control del error global a través del criterio del error local}
\begin{definition}
Dado el problema de valor inicial 
$$
\left\{
\begin{array}{lll}
y'(t) = f(t,y) & & \\
y(t_0) = y_0 & &
\end{array}
\right.
$$
 que queremos resolver, y un método numérico en diferencias
\begin{equation}
\label{eq6}
\left\{
\begin{array}{lll}
\omega_0 = \alpha & & \\
\omega_{i+1} = \omega_i + h \phi(t_i, \omega_i, h) & & 
\end{array}
\right.
\end{equation}
definimos la \textbf{solución local} $z_n$ como la solución del pvi
$$
\left\{
\begin{array}{lll}
z_n'(t) = f(t,z_n) & & \\
z_n(t_n) = \omega_n & &
\end{array}
\right.
$$
\end{definition}

Este último pvi es el que realmente resolvemos en cada paso que damos en un método:
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.5]
          \draw[->] (-0.5, 0) -- (3,0) node[right] {$t$};
          \draw[->] (0,-0.5) -- (0,2.5) node[above] {$y$};
          \draw [fill,red] (0.5,0.5) circle [radius=0.05] node[red,right] {$t_0$}; %primer punto del método
          \draw [fill, green] (1,1.3) circle [radius=0.05] node[green,right] {$t_1$}; %2º punto del método
          \draw [fill, green] (1.7,2) circle [radius=0.05] node[green,right] {$t_2$}; %3º punto del método
          
          \draw[red,-] (0.5,0.5)to[bend left] (3,1.5) ;
          \draw[green,-,dashed] (0.5,0.85)to[bend left] (3,1.8) ;
          \draw[green,-,dashed] (0.5,1.15)to[bend left] (3,2.05) ;
          \draw[green,-] (0.5,0.5) -- (1,1.3) ;
          \draw[green,-] (1,1.3) -- (1.7,2) ;
\end{tikzpicture}
    \caption{Pvi en cada paso 1D}
\end{figure}
que en este dibujo serían las curvas de color verde (el problema inicial sería la curva roja).
\begin{theorem}
Supongamos que el método de integración  (\ref{eq6}) verifica (en cada paso)
$$
\begin{array}{ll}
\|z_n(t_n+h) - \omega_{n+1}\| \leq \varepsilon\cdot h & \textnormal{(criterio de error local)}
\end{array}
$$
para cada tolerancia $\varepsilon$ dada.\\
Entonces, si $f$ es lipschitziana con constante $k$, se tiene que 
$$
\|y(t_n)-\omega_n\| \leq e^{k(t_n-a)}\|y(b)-\omega_0\| + \frac{e^{k(t_n-a)}}{k}\varepsilon.
$$
\end{theorem}
\begin{remark}
Recuerda a la cota que obtuvimos para el método de Euler (si $k$ no es muy grande, para $b-a\approx 1$ es aceptable).
\end{remark}
\begin{remark}
Es independiente del método.
\end{remark}
\begin{remark}
Observar que, si asumimos que $z_n(t_n) \approx y(t_n) \approx \omega_n $, este criterio consiste en mantener
$$
\|\tau_{i+1}(h)\| = \frac{\| y(t_{i+1}) - (y(t_i) + h\phi(t_i, \omega_i, h)) \|}{h} \approx \frac{\| z_i(t_{i+1}) - \omega_{i+1} \|}{h} \leq \varepsilon
$$
es decir, mantener el error local de truncamiento por debajo de la tolerancia.
\end{remark}

\newpage
\subsection{Control del error por extrapolación de Richardson}
\begin{theorem}
Supongamos que el método en diferencias 
\begin{equation*}
\left\{
\begin{array}{lll}
\omega_0 = \alpha & & \\
\omega_{i+1} = \omega_i + h_i \phi(t_i, \omega_i, h_i) & & i = 0,1,\dots, N-1
\end{array}
\right.
\end{equation*}
verifica (en todo paso): \astfootnote{Usando la notación $\omega_{i+1} = y_h(t_i+h)$.}
\begin{equation}
\label{eq7}
z_i(t_i+h) = y_h(t_i+h) + c\cdot z_i^{(k+1)}(t_i)h^{k+1} + \mathcal{O}(h^{k+2})
\end{equation}
donde $c$ es una constante. Entonces se tiene:
\begin{equation}
\label{eq8}
z_i(t_i+h) = y_{\frac{h}{2}}(t_i+h) + 2c\cdot z_i^{(k+1)}(t_i)\left(\frac{h}{2}\right)^{k+1} + \mathcal{O}(h^{k+2})
\end{equation}
\end{theorem}
\begin{remark}
En este caso el error local de truncamiento es de orden $k$. Por ejemplo, el método de Euler tiene $k=1$ y el de Runge-Kutta de orden 4 tiene $k=4$.
\end{remark}

Haciendo $(\ref{eq8})\cdot 2^k - (\ref{eq7})$ obtenemos una fórmula para \textbf{mejorar la solución}:
\begin{equation}
\label{eq9}
z_i(t_i+h) = \frac{2^ky_{\frac{h}{2}}(t_i+h)-y_h(t_i+h)}{2^k-1}+\mathcal{O}(h^{k+2})
\end{equation}
Y restando a eso $y_h(t_i+h)$, obtenemos una fórmula para \textbf{aproximar el error local de truncamiento}:
\begin{equation}
\label{eq10}
z_i(t_i+h) -y_h(t_i+h)= \frac{2^k}{2^k-1}\left(y_{\frac{h}{2}}(t_i+h)-y_h(t_i+h)\right)+\mathcal{O}(h^{k+1})
\end{equation}
\begin{example}
\begin{itemize}
    \item Euler ($k=1$)  
    $$
    elt \approx 2\left(y_{\frac{h}{2}}(t_i+h)-y_h(t_i+h)\right)
    $$
    \item RK4 ($k=4$).
    $$
    elt \approx \frac{16}{15}\left(y_{\frac{h}{2}}(t_i+h)-y_h(t_i+h)\right)
    $$
\end{itemize}
\end{example}

\newpage
\subsection{Estimación local del paso con Richardson}
Un método ideal de la ecuación de diferencias 
\begin{equation*}
\left\{
\begin{array}{lll}
\omega_0 = \alpha & & \\
\omega_{i+1} = \omega_i + h_i \phi(t_i, \omega_i, h_i) & & i = 0,1,\dots, N-1
\end{array}
\right.
\end{equation*}
para aproximar la solución $z_i(t)$ al problema de valor inicial
\begin{equation*}
\left\{
\begin{array}{lll}
z_i'(t) = f(t,z_i) & & \\
z_i(t_i) = \omega_i & &
\end{array}
\right.
\end{equation*}
deberá tener la propiedad de que, con una tolerancia $\varepsilon >0$, se puede mantener $$|z_i(t_i+h) - y_h(t_i+h)| = |z_i(t_i+h) - \omega_{i+1}| \leq \varepsilon\cdot h$$ para todo $i=0,1,\dots N-1$ si queremos controlar el error local, ya que este es el error introducido en la solución en el punto $t_{i+1} = t_i +h$ si asumimos que la solución $y_n$ en el punto anterior, $t_n$, es la solución exacta. \\
La manera más fácil de estimar este error es usando la extrapolación de Richardson de la siguiente manera: 
\begin{enumerate}
\item Resolvemos el pvi dos veces en el intervalo $[t_0,b]$ con tamaños de paso $2h$ y $h$ \astfootnote{P.Esquembre usa $h$ y $\frac{h}{2}$.}.
\item Estimamos el error, $\tilde{e}_r(h) = \| z_i(t_i+h_i) - \omega_{i+1} \|$, con extrapolación de Richardson.
\item Si el error estimado es menor o igual que $\varepsilon \cdot h$ ($\tilde{e}_r(h)\leq \varepsilon \cdot h$) entonces el paso es el adecuado y pasamos a aproximar $\omega_{i+1}$ con la fórmula (\ref{eq9}).
\item Si no, ¿cuál tendría que haber sido el paso $h^* = qh$ para que el error sí fuese menor o igual que $\varepsilon \cdot h$? Lo vemos:
$$
\|z_i(t_h + qh) - y_{qh}(t_i+qh)\| \approx C(qh)^{k+1} = Cq^{k+1}h^{k+1} \approx
$$
$$
\approx q^{k+1}\cdot \|z_i(t_h + h) - y_h(t_i+h)\| = q^{k+1}\cdot \tilde{e}_r(h)\leq \varepsilon qh
$$
\\Donde hemos usado que $Ch^{k+1} \approx \|z_i(t_i + h) - y_h(t_i+h)\|$. Obtenemos, entonces, que debe ser
$$
q \leq \left(\frac{\varepsilon h}{\tilde{e}_r(h)}\right)^{\frac{1}{k}}
$$
Por tanto, cuando $\tilde{e}_r(h)> \varepsilon \cdot h$ lo que haremos será tomar $q =\left(\frac{\varepsilon h}{\tilde{e}_r(h)}\right)^{\frac{1}{k}} $ y $h = q\cdot h$ para volver a dar el paso con un $h$ que esperamos nos permita mantener $\tilde{e}_r(h)\leq \varepsilon\cdot h$. Este proceso se repetirá hasta que lleguemos a un $h$ que cumpla esto último.
\end{enumerate}
\begin{remark}
El coste de estimar el error de esta manera supone, aproximadamente, un incremento del 50\% en la cantidad de cómputo, comparándolo con solo calcular $y_h$.\\
Puede parecer un coste demasiado alto, pero generalmente merece la pera excepto para los problemas que más tiempo emplean. 
\end{remark}  
\begin{remark}
Este método también nos dice cuándo podemos coger un paso más grande y seguir manteniendo la tolerancia: si $q < \left(\frac{\varepsilon h}{\tilde{e}_r(h)}\right)^{\frac{1}{k}}$, podemos tomar $h=\left(\frac{\varepsilon h}{\tilde{e}_r(h)}\right)^{\frac{1}{k}}h$, que es más grande que el paso anterior, y se seguiría manteniendo $\tilde{e}_r(h)\leq \varepsilon\cdot h$.
\end{remark}

\subsection{Método de Runge-Kutta-Fehlberg}
Para ilustrar este método, supogamos que tenemos dos métodos de aproximación. El primero es un Runge-Kutta de orden $n$
$$
y(t_{i+1}) = y(t_i) + h\phi (t_i, y(t_i), h) + \mathcal{O}(h^{n+1})
$$
que produce las aproximaciones
$$
\left\{
\begin{array}{lll}
\omega_0 = \alpha & & \\
\omega_{i+1} = \omega_i + h\phi(t_i, \omega_i, h) & & i >0
\end{array}
\right.
$$
\\con error de truncamiento $\tau_{i+1}(h) = \mathcal{O}(h^n)$, y el segundo es otro método de Runge-Kutta pero de orden $n+1$
$$
y(t_{i+1}) = y(t_i) + h\tilde{\phi}(t_i, y(t_i), h) + \mathcal{O}(h^{n+2})
$$
que produce las aproximaciones
$$
\left\{
\begin{array}{lll}
\tilde{\omega_0} = a & & \\
\tilde{\omega}_{i+1} = \tilde{\omega}_i + h\tilde{\phi}(t_i, \tilde{\omega}_i, h) & & i >0
\end{array}
\right.
$$
\\con error de truncamiento $\tau_{i+1}(h) = \mathcal{O}(h^{n+1})$.\\
Suponiendo $\omega_i \approx y(t_i) \approx \tilde{\omega}_i$ y tomando un $h$ fijo tenemos que
$$
\begin{array}{l}
\tau_{i+1}(h) = \frac{y(t_{i+1}) - y(t_i)}{h} - \phi(t_i, y(t_i), h) \approx  \frac{y(t_{i+1}) - \omega_i}{h} - \phi(t_i, \omega_i, h) =\\
\\
 =\frac{y(t_{i+1}) - (\omega_i+ - h\phi(t_i, \omega_i, h))}{h} = \frac{1}{h}(y(t+i)-\omega_{i+1})
\end{array}
$$
Y, análogamente, se tiene que 
$$
\tilde{\tau}_{i+1}(h) \approx \frac{1}{h}(y(t+i)-\tilde{\omega}_{i+1})
$$
Si ahora sumamos y restamos el término $\frac{1}{h}\tilde{\omega}_{i+1}$ en la aproximación obtenida para $\tau_{i+1}(h)$, llegamos a que
$$
\tau_{i+1}(h) = \frac{1}{h}[(y(t_{i+1})-\tilde{\omega}_{i+1}) + (\tilde{\omega}_{i+1} - \omega_{i+1})] = \tilde{\tau}_{i+1}(h) + \frac{1}{h}(\tilde{\omega}_{i+1}-\omega_{i+1})
$$
Ahora, como $\tilde{\tau}_{i+1}(h)$ es una $\mathcal{O}(h^{n+1})$, podemos aproximar el error local de truncamiento del método de orden $n$ como
$$
\tau_{i+1}(h) \approx \frac{1}{h}(\tilde{\omega}_{i+1}-\omega_{i+1})
$$
\\De manera que la aproximación del error local de truncamiento al tomar el paso de tamaño $qh$ es
$$
\tau_{i+1}(qh)\approx C(qh)^n = Cq^nh^n \approx q^n \tau_{i+1}(h) \approx \frac{q^n}{h}(\tilde{\omega}_{i+1}-\omega_{i+1}) 
$$
donde hemos utilizado que $\tau_{i+1}(h) \approx Ch^n$ por ser $\tau_{i+1}(h)$ una $\mathcal{O}(h^n)$.
\\Para establecer la cota del error local de truncamiento por la tolerancia $\varepsilon$ tendríamos, por tanto, que escoger $q$ tal que
$$
q \leq \left(\frac{\varepsilon h}{|\tilde{\omega}_{i+1} - \omega_{i+1}|}\right)^{\frac{1}{n}}
$$
Un método muy usado que utiliza esta última desigualdad para controlar el error local de truncamiento es el \textbf{método de Runge-Kutta-Felhberg}, que pasamos a introducir a continuación.\\

Si eligiésemos dos métodos arbitrarios de Runge-Kutta de cuarto y quinto orden para aplicar lo que acabamos de ver, necesitaríamos, como mínimo, 10 evaluaciones de la función $f$ (4 por el método de orden 4 y 6 por el de orden 5). Sin embargo, \textbf{Erwin Fehlberg} encontró 2 métodos de Runge-Kutta de estos órdenes que se pueden anidar de manera que se necesiten tan solo 6 evaluaciones de $f$. \\
El \textbf{método de Runge-Kutta-Fehlberg} consiste, por tanto, en emplear Runge-Kutta con el error local de truncamiento de quinto orden
$$
\tilde{\omega}_{i+1} = \omega_i + \frac{16}{135}k_1 + \frac{6656}{12825}k_3 + \frac{28561}{56430}k_4 - \frac{9}{50}k_5 + \frac{2}{55}k_6
$$
para estimar el error local en un método de Runge-Kutta de cuarto orden dado por 
$$
\omega_{i+1} = \omega_i + \frac{25}{216}k_1 + \frac{1408}{2565}k_3 + \frac{2197}{4104}k_4 - \frac{1}{5}k_5 
$$
donde 
$$
\begin{array}{l}
\\
k_1 =  hf\left(t_i, \omega_i\right)\\
\\
k_2 =  hf\left(t_i + \frac{h}{4}, \omega_i + \frac{1}{4}k_1\right)\\
\\
k_3 =  hf\left(t_i + \frac{3h}{8}, \omega_i + \frac{3}{32}k_1 +  \frac{9}{32}k_2\right)\\
\\
k_4 = hf\left(t_i + \frac{12h}{13}, \omega_i + \frac{1932}{2197}k_1 -   \frac{7200}{2197}k_2 +  \frac{7296}{2197}k_3\right) \\
\\
k_5 = hf\left(t_i + h, \omega_i + \frac{439}{216}k_1 - 8k_2 +  \frac{3680}{513}k_3 - \frac{845}{4104}k_4\right) \\
\\
k_6 = hf\left(t_i + \frac{h}{2}, \omega_i - \frac{8}{27}k_1 + 2k_2 -  \frac{3544}{2565}k_3 + \frac{1859}{4104}k_4 - \frac{11}{40}k_5\right)
\end{array}
$$
\\ //aquí falta algoritmo completo
